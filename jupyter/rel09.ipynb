{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REL 9: How are you planning for disaster recovery?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the ideas above are great for improving reliability and preventing outages, but what about the day they actually do happen? That is when a tested disaster recovery plan pays off. Like insurance, you’d rather waste that resource than using, but it is important to have the cost and coverage that works better for your scenario.\n",
    "\n",
    "For the purpose of this chapter, a disaster can be defined as an unexpected irrecoverable failure. Systems are down and not restarting, data is being lost, people are stressed. A disaster recovery plan balances the speed and cost of getting back up. Two DR metrics are frequently discussed:\n",
    "\n",
    "Recovery Time Objective (RTO): How long it should take for systems to back up. For example, a recovering a database backup from cold storage could take a couple hours, but only a couple minutes to recover from a secondary replica.\n",
    "\n",
    "Recovery Point Objective (RPO): How much data can be lost, usually in a time unit. For example, if a daily backup is taken, up to 24 hours of data can be lost in a disaster. RPO is always longer than RTO, as data is lost during the downtime plus the time since last copy. \n",
    "\n",
    "It may be very expensive to get this numbers down in traditional IT, as it requires independent facilities and redundant networking. Amazon Global Infrastructure improves that significantly by offering at least two segregated availability zones per region and several regions worldwide at similar costs. However, it is still up to the architect to design the appropriate disaster recovery scenario for his demand and pocket. The “Using Amazon Web Services for Disaster Recovery” whitepaper suggests the following alternatives:\n",
    "\n",
    "## Backup and Restore\n",
    "\n",
    "Loosing data can be truly catastrophic, so having a recent and tested backup is probably the most given advice in IT. Amazon S3 can store the backup files reliably and efficiently, specially using Standard – IA and Glacier. However, there are other services and tools that can help getting the data there. EBS Snapshots are copies of EBS volumes stored on S3, taken incrementally to minimize the time and costs. Although snapshots are incremental, they are independent, EBS will keep all the data required to restore a snapshot. Data blocks are safely deleted only when there are no more snapshots referencing them.\n",
    "\n",
    "This approach costs only for data storage, as no redundant servers are in place, but it may take a while to run Glacier data retrievals and resurrect servers from snapshots. This can cost a lot in terms of RPO, as data received since the last backup may be lost, and RTO, as large backups take long to restore.\n",
    "\n",
    "## Pilot Light\n",
    "\n",
    "Like the small everlasting flame in gas heaters, a pilot light strategy will keep only the core of the system turned on in redundancy and bring up the rest when needed. Usually, this means having the data continuously replicated, but no application server or upper tiers. It still may take some time to start application servers and get back to users, but data loss will be restricted to the time since last successful replication.\n",
    "\n",
    "This approach is more expensive than backup and restore, as replication incurs in costs, but can reduce RPO down to the replication frequency. However, complex systems may take a long time to bootstrap and stabilize, leading to excessive RTO.\n",
    "\n",
    "## Warm Standby\n",
    "\n",
    "Keeping a small clone environment for disaster recovery can make recovery much faster. The on-demand resource provisioning in auto-scaling or serverless architectures makes the cost of running a standby environment negligible. When traffic shifts, the standby environment can scale to accommodate full load. \n",
    "The RPO for warm standby is the same, as that is defined by the replication type and frequency. But RTO can be much smaller, as resources are already up and running. However, some performance degradation and errors may still be perceived while the new environment scales. Particularly for high-traffic systems, throwing a firehose of traffic on a cold environment can trigger yet more failures.\n",
    "\n",
    "## Multi-Site\n",
    "\n",
    "“Production” can be composed of many environments at full capacity and load balanced. In this case, the traffic from the failed environment is divided across the remaining copies and even large-scale events can be managed. However, the viability of multi-site replication and running multiple environments is highly dependent on the system architecture. But for the systems that do support multi-master high availability, both RTO and RPO can be kept minimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
